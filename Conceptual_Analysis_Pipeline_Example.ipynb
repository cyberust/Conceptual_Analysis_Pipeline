{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWah3yvoLTYW"
      },
      "outputs": [],
      "source": [
        "# ===== 0. 必要なライブラリのインポート =====\n",
        "print(\"Installing necessary libraries...\")\n",
        "# !pip install pdfminer.six bertopic spacy umap-learn hdbscan scikit-learn pandas networkx matplotlib plotly openpyxl markdown-it-py python-frontmatter sentence-transformers keybert\n",
        "# print(\"Downloading spaCy model...\")\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "import re\n",
        "from io import StringIO\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from bertopic import BERTopic\n",
        "import umap\n",
        "import hdbscan\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import itertools\n",
        "import numpy as np\n",
        "import os\n",
        "import glob # ファイル検索用\n",
        "from markdown_it import MarkdownIt # マークダウン解析用\n",
        "import frontmatter # マークダウンのフロントマター解析用\n",
        "\n",
        "# 追加する可能性のあるライブラリ\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from keybert import KeyBERT\n",
        "# from textblob import TextBlob # 感情分析用 (オプション)\n",
        "\n",
        "print(\"Libraries imported.\")\n",
        "\n",
        "# ===== A. 分析ツールセット 設定フラグとパラメータ =====\n",
        "# これらをTrue/Falseで切り替えて、使用するツールを選択\n",
        "USE_MARKDOWN_INPUT = True # Trueにするとマークダウンファイルも読み込む\n",
        "USE_NGRAMS = True\n",
        "NGRAM_MIN_COUNT = 5\n",
        "NGRAM_THRESHOLD = 10\n",
        "USE_POS_FILTERING = True\n",
        "ALLOWED_POS = ['NOUN', 'PROPN', 'ADJ'] # 例: 名詞、固有名詞、形容詞\n",
        "USE_NER_EXTRACTION = False # 必要に応じてTrueに\n",
        "ALLOWED_NER_LABELS = ['ORG', 'PRODUCT', 'PERSON', 'WORK_OF_ART', 'EVENT'] # 例\n",
        "USE_SENTENCE_BERT_EMBEDDINGS = True # Word Embeddings (Sentence-BERT)\n",
        "USE_KEYBERT_EXTRACTION = True # KeyBERTによるキーワード抽出\n",
        "USE_WATCHLIST_ANALYSIS = True # 注目ワードリスト分析\n",
        "\n",
        "# Sentence-BERT モデル名\n",
        "SBERT_MODEL_NAME = 'all-MiniLM-L6-v2' # または 'stsb-roberta-large', 'paraphrase-multilingual-mpnet-base-v2' 等\n",
        "\n",
        "# 注目ワードリスト (前処理後の形に近いものを想定するか、前処理も通す)\n",
        "WATCHLIST_KEYWORDS_RAW = [\"terraforming governance\", \"memoryfield feedback\", \"semantic divergence\", \"構造知性\"] # 例\n",
        "\n",
        "# ===== 1. 関数定義 (PDF抽出、CV分割 - 既存) =====\n",
        "# (extract_text_from_pdf_pdfminer, split_cv_by_company は既存のものをそのまま使用)\n",
        "def extract_text_from_pdf_pdfminer(pdf_path):\n",
        "    # (既存のコード)\n",
        "    if not os.path.exists(pdf_path):\n",
        "         print(f\"Error in extract_text: PDF file not found at {pdf_path}\")\n",
        "         return None\n",
        "    output_string = StringIO()\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as in_file:\n",
        "            parser = PDFParser(in_file)\n",
        "            doc = PDFDocument(parser)\n",
        "            rsrcmgr = PDFResourceManager()\n",
        "            laparams = LAParams(line_margin=0.5, boxes_flow=0.5)\n",
        "            device = TextConverter(rsrcmgr, output_string, laparams=laparams)\n",
        "            interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "            for page in PDFPage.create_pages(doc):\n",
        "                interpreter.process_page(page)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error in extract_text: PDF file not found (during open) at {pdf_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF with pdfminer.six: {e}\")\n",
        "        return None\n",
        "    return output_string.getvalue()\n",
        "\n",
        "def split_cv_by_company(text):\n",
        "    # (既存のコード)\n",
        "    print(\"\\n--- DEBUG (split_cv): Entering function ---\")\n",
        "    if not text:\n",
        "        print(\"--- DEBUG (split_cv): Input text is empty or None.\")\n",
        "        return []\n",
        "    lines = text.split('\\n')\n",
        "    prof_exp_start_line_index = -1\n",
        "    search_term = \"professional experience\"\n",
        "    print(\"--- DEBUG (split_cv): Searching for 'Professional Experience' header ---\")\n",
        "    for i, line in enumerate(lines):\n",
        "        processed_line = ' '.join(line.lower().split())\n",
        "        if search_term in processed_line.replace('', ''):\n",
        "             print(f\"--- DEBUG (split_cv): Found header at line index {i}\")\n",
        "             prof_exp_start_line_index = i + 1\n",
        "             break\n",
        "    if prof_exp_start_line_index == -1:\n",
        "        print(\"--- DEBUG (split_cv): 'Professional Experience' header not found.\")\n",
        "        return []\n",
        "    pattern_job_date_line = (\n",
        "        r\"^[A-Za-z\\s,/()&]+,\"\n",
        "        r\"\\s*\"\n",
        "        r\"(?:\"\n",
        "            r\"(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{4}\"\n",
        "            r\"(?:\\s*[-–]\\s*\"\n",
        "            r\"(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|\\w+)\"\n",
        "            r\"(?:\\s+\\d{4})?)?\"\n",
        "        r\"|\"\n",
        "            r\"\\d{4}\\s*[-–]\\s*\\d{4}\"\n",
        "        r\")$\"\n",
        "    )\n",
        "    job_header_indices = []\n",
        "    print(f\"--- DEBUG (split_cv): Searching for job/date pattern from line index {prof_exp_start_line_index} ---\")\n",
        "    for i in range(prof_exp_start_line_index, len(lines)):\n",
        "        line = lines[i]\n",
        "        line_stripped = line.strip().rstrip('').strip()\n",
        "        if line_stripped:\n",
        "             match_result = re.match(pattern_job_date_line, line_stripped)\n",
        "             if ',' in line_stripped and any(char.isdigit() for char in line_stripped):\n",
        "                 if match_result:\n",
        "                      print(f\"DEBUG (split_cv) Line {i}: MATCH! -> '{line_stripped}'\")\n",
        "                      job_header_indices.append(i)\n",
        "                 else:\n",
        "                      if not re.match(r\"^[\\d•]\", line_stripped):\n",
        "                           print(f\"DEBUG (split_cv) Line {i}: NO MATCH -> '{line_stripped}'\")\n",
        "    print(f\"--- DEBUG (split_cv): Finished pattern search. Found {len(job_header_indices)} potential header lines.\")\n",
        "    if not job_header_indices:\n",
        "        print(\"--- DEBUG (split_cv): No job/date lines matched the pattern.\")\n",
        "        return []\n",
        "    sections = [];\n",
        "    for i in range(len(job_header_indices)):\n",
        "        header_index = job_header_indices[i]; section_start = header_index - 1\n",
        "        while section_start > prof_exp_start_line_index -1 and not lines[section_start].strip(): section_start -= 1\n",
        "        prev_header_index = job_header_indices[i-1] if i > 0 else prof_exp_start_line_index - 1\n",
        "        section_start = max(prev_header_index + 1, section_start)\n",
        "        if i + 1 < len(job_header_indices):\n",
        "            next_header_index = job_header_indices[i+1]; section_end = next_header_index - 1\n",
        "            while section_end > header_index and not lines[section_end].strip(): section_end -= 1\n",
        "            section_end += 1; next_section_start = next_header_index -1\n",
        "            while next_section_start > header_index and not lines[next_section_start].strip(): next_section_start -=1\n",
        "            section_end = min(section_end, next_section_start)\n",
        "        else: section_end = len(lines)\n",
        "        section_lines = lines[section_start:section_end]\n",
        "        if section_lines and not all(s.strip() == '' for s in section_lines):\n",
        "             clean_section = \"\\n\".join(section_lines).strip().rstrip('').strip(); sections.append(clean_section)\n",
        "    print(f\"--- DEBUG (split_cv): Returning {len(sections)} sections. ---\")\n",
        "    return sections\n",
        "print(\"Helper functions defined.\")\n",
        "\n",
        "\n",
        "# ===== 1.5. 新しい関数定義 (マークダウン処理、前処理ツール) =====\n",
        "def clean_markdown_text(md_text):\n",
        "    \"\"\"マークダウンテキストから純粋なテキストを抽出（簡易版）\"\"\"\n",
        "    # HTMLタグの除去\n",
        "    text = re.sub(r'<[^>]+>', ' ', md_text)\n",
        "    # Markdown特有の記号の除去 (見出し、リスト、強調、リンクなど)\n",
        "    text = re.sub(r'^#+\\s*', '', text, flags=re.MULTILINE)  # 見出し\n",
        "    text = re.sub(r'^\\s*[\\*\\-\\+]\\s+', '', text, flags=re.MULTILINE) # リストマーカー\n",
        "    text = re.sub(r'(\\*\\*|__)(.*?)(\\*\\*|__)', r'\\2', text) # 太字\n",
        "    text = re.sub(r'(\\*|_)(.*?)(\\*|_)', r'\\2', text)     # 斜体\n",
        "    text = re.sub(r'`(.*?)`', r'\\1', text)               # インラインコード\n",
        "    text = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', text)     # リンク (リンクテキストのみ保持)\n",
        "    text = re.sub(r'!\\[(.*?)\\]\\(.*?\\)', r'\\1', text)    # 画像 (altテキストのみ保持)\n",
        "    # 連続する空白を一つに\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def extract_text_from_markdown(md_file_path):\n",
        "    \"\"\"マークダウンファイルからテキストを抽出\"\"\"\n",
        "    if not os.path.exists(md_file_path):\n",
        "        print(f\"Error: Markdown file not found at {md_file_path}\")\n",
        "        return None\n",
        "    try:\n",
        "        with open(md_file_path, 'r', encoding='utf-8') as f:\n",
        "            post = frontmatter.load(f) # フロントマターも考慮する場合\n",
        "            content = post.content\n",
        "            # content = f.read() # フロントマターを考慮しない場合\n",
        "        # Markdownをパースしてテキストのみ抽出 (markdown-it-py を使用)\n",
        "        md = MarkdownIt()\n",
        "        html_content = md.render(content) # 一旦HTMLに変換\n",
        "        text_content = clean_markdown_text(html_content) # HTMLタグやMD記号を除去\n",
        "        # または、より単純な clean_markdown_text(content) でも可\n",
        "        return text_content\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading Markdown file {md_file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_text_with_spacy(text, nlp, custom_stop_words=None):\n",
        "    \"\"\"spaCyを用いた高度な前処理（品詞フィルタリング、NER抽出を含む）\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return []\n",
        "    doc = nlp(text)\n",
        "    tokens = []\n",
        "\n",
        "    # 固有表現抽出 (USE_NER_EXTRACTIONがTrueの場合)\n",
        "    if USE_NER_EXTRACTION:\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in ALLOWED_NER_LABELS:\n",
        "                # 固有表現はそのままの形で追加（レンマ化しない方が良い場合も）\n",
        "                tokens.append(ent.text.lower().strip())\n",
        "\n",
        "    # 通常のトークン処理\n",
        "    for token in doc:\n",
        "        is_stop = token.is_stop or (custom_stop_words and token.lemma_.lower() in custom_stop_words)\n",
        "        if not is_stop and not token.is_punct and len(token.text) > 1:\n",
        "            if USE_POS_FILTERING:\n",
        "                if token.pos_ in ALLOWED_POS:\n",
        "                    tokens.append(token.lemma_.lower())\n",
        "            else:\n",
        "                tokens.append(token.lemma_.lower())\n",
        "    return tokens\n",
        "\n",
        "def generate_ngrams(processed_token_list, min_count=5, threshold=10):\n",
        "    \"\"\"N-gram (bigram, trigram) を生成\"\"\"\n",
        "    if not USE_NGRAMS or not processed_token_list:\n",
        "        return processed_token_list\n",
        "\n",
        "    # Phrasesモデルはリストのリストを入力とする\n",
        "    docs_for_phrases = [doc.split() for doc in processed_token_list if isinstance(doc, str)]\n",
        "    if not docs_for_phrases:\n",
        "        return processed_token_list\n",
        "\n",
        "    try:\n",
        "        bigram_model = Phrases(docs_for_phrases, min_count=min_count, threshold=threshold)\n",
        "        trigram_model = Phrases(bigram_model[docs_for_phrases], threshold=threshold) # min_countはbigramで効いている\n",
        "        bigram_phraser = Phraser(bigram_model)\n",
        "        trigram_phraser = Phraser(trigram_model)\n",
        "\n",
        "        ngram_docs = []\n",
        "        for doc_tokens in docs_for_phrases:\n",
        "            ngram_docs.append(\" \".join(trigram_phraser[bigram_phraser[doc_tokens]]))\n",
        "        return ngram_docs\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating ngrams: {e}\")\n",
        "        return processed_token_list # エラー時は元のリストを返す\n",
        "\n",
        "\n",
        "# ===== 2. データ読み込みと準備 =====\n",
        "print(\"\\n--- Loading Data Sources ---\")\n",
        "# --- CVデータ (PDFとマークダウンの両方を考慮) ---\n",
        "cv_text_sources = [] # 生のテキストソースをここにためる\n",
        "\n",
        "# PDFからのCVデータ (既存)\n",
        "pdf_file_path = '/content/drive/MyDrive/Yasuyuki_Sakane_20250403.pdf' # ★正しいファイルパス★\n",
        "print(f\"Checking for CV PDF file at: {pdf_file_path}\")\n",
        "if os.path.exists(pdf_file_path):\n",
        "    print(\"CV PDF file found. Attempting to extract text...\")\n",
        "    cv_text_pdf = extract_text_from_pdf_pdfminer(pdf_file_path)\n",
        "    if cv_text_pdf:\n",
        "        cv_text_sources.append({\"source\": pdf_file_path, \"text\": cv_text_pdf, \"type\": \"pdf_cv\"})\n",
        "else:\n",
        "    print(f\"Warning: CV PDF file not found at {pdf_file_path}\")\n",
        "\n",
        "# マークダウンからのCVデータ (新規追加)\n",
        "if USE_MARKDOWN_INPUT:\n",
        "    markdown_cv_folder = '/content/drive/MyDrive/CV_Markdown/' # ★マークダウンCVファイルがあるフォルダ★\n",
        "    md_cv_files = glob.glob(os.path.join(markdown_cv_folder, \"*.md\"))\n",
        "    print(f\"Found {len(md_cv_files)} Markdown CV files in {markdown_cv_folder}\")\n",
        "    for md_file in md_cv_files:\n",
        "        print(f\"Reading Markdown CV from: {md_file}\")\n",
        "        md_text = extract_text_from_markdown(md_file)\n",
        "        if md_text:\n",
        "            cv_text_sources.append({\"source\": md_file, \"text\": md_text, \"type\": \"md_cv\"})\n",
        "\n",
        "# CVテキストの分割処理 (複数のCVソースに対応)\n",
        "cv_sections = []\n",
        "for cv_source in cv_text_sources:\n",
        "    if cv_source[\"text\"]:\n",
        "        print(f\"\\nAttempting to split text from {cv_source['source']} into sections...\")\n",
        "        # split_cv_by_company はPDFのレイアウトを前提としているため、マークダウンには不向きな場合がある\n",
        "        # マークダウンの場合は、構造（例：H2見出しごと）で分割するロジックを別途検討するか、全体を一つのセクションとする\n",
        "        if cv_source[\"type\"] == \"pdf_cv\":\n",
        "            sections = split_cv_by_company(cv_source[\"text\"])\n",
        "            if sections:\n",
        "                cv_sections.extend(sections)\n",
        "            else:\n",
        "                print(f\"Warning: PDF CV splitting resulted in 0 sections for {cv_source['source']}. Using full text.\")\n",
        "                cv_sections.append(cv_source[\"text\"]) # 分割失敗時は全文を1セクションとして追加\n",
        "        elif cv_source[\"type\"] == \"md_cv\":\n",
        "            # マークダウンの場合、単純に全文を1セクションとして扱うか、\n",
        "            # またはマークダウンの構造（例：特定のレベルの見出し）で分割するロジックをここに追加\n",
        "            print(f\"Info: For Markdown CV {cv_source['source']}, using full text as one section. Consider custom splitting if needed.\")\n",
        "            cv_sections.append(cv_source[\"text\"])\n",
        "\n",
        "print(f\"Total CV sections collected: {len(cv_sections)}\")\n",
        "if not cv_sections:\n",
        "    print(\"Warning: No CV sections were loaded or processed.\")\n",
        "\n",
        "\n",
        "# --- ジャーナルデータ (既存) ---\n",
        "journal_file_path = '/content/drive/MyDrive/Journal_02_20250504.csv'\n",
        "# (既存のジャーナルデータ読み込みコード)\n",
        "journal_docs = []\n",
        "try:\n",
        "    df_journal = pd.read_csv(journal_file_path, header=None)\n",
        "    if df_journal.shape[1] > 2:\n",
        "         journal_docs = df_journal.iloc[:, 2].dropna().astype(str).tolist()\n",
        "         print(f\"Loaded {len(journal_docs)} journal entries from CSV (Column C).\")\n",
        "    else: print(f\"Error: Column C (index 2) not found in {journal_file_path}. Check CSV structure.\")\n",
        "except FileNotFoundError: print(f\"Journal CSV file not found at {journal_file_path}\")\n",
        "except Exception as e: print(f\"Error reading journal CSV: {e}\")\n",
        "\n",
        "\n",
        "# --- 用語リスト (既存) ---\n",
        "buzzword_file_path = '/content/drive/MyDrive/BuzzWords2024_Pillar.xlsx'\n",
        "# (既存の用語リスト読み込みコード)\n",
        "my_keywords_from_list_set = set()\n",
        "keyword_metadata = {}\n",
        "try:\n",
        "    all_sheets_df = pd.read_excel(buzzword_file_path, sheet_name=None)\n",
        "    print(f\"Read {len(all_sheets_df)} sheets from Buzzwords Excel.\")\n",
        "    for sheet_name, df_sheet in all_sheets_df.items():\n",
        "        term_col_index = 1 # B列を想定\n",
        "        if df_sheet.shape[1] > term_col_index and not df_sheet.iloc[:, term_col_index].isnull().all():\n",
        "            sheet_keywords = df_sheet.iloc[:, term_col_index].dropna().astype(str).str.lower().str.strip().tolist()\n",
        "            my_keywords_from_list_set.update(sheet_keywords)\n",
        "            col_indices = {'context': 2, 'cat_major': 3, 'cat_mid': 4, 'cat_minor': 5, 'sentiment': 7, 'topic': 8}\n",
        "            available_meta_cols = {key: idx for key, idx in col_indices.items() if df_sheet.shape[1] > idx}\n",
        "            for index, row in df_sheet.iterrows():\n",
        "                 term_raw = row.iloc[term_col_index]\n",
        "                 if pd.notna(term_raw):\n",
        "                      term = str(term_raw).lower().strip()\n",
        "                      if term:\n",
        "                           if term not in keyword_metadata: keyword_metadata[term] = {}\n",
        "                           keyword_metadata[term]['pillar'] = keyword_metadata[term].get('pillar', []) + [sheet_name]\n",
        "                           for key, idx in available_meta_cols.items():\n",
        "                                if key not in keyword_metadata[term] or pd.notna(row.iloc[idx]):\n",
        "                                     keyword_metadata[term][key] = row.iloc[idx]\n",
        "    my_keywords_from_list = sorted(list(my_keywords_from_list_set))\n",
        "    print(f\"Loaded {len(my_keywords_from_list)} unique keywords from all Excel sheets.\")\n",
        "except FileNotFoundError: print(f\"Buzzwords Excel file not found at {buzzword_file_path}\")\n",
        "except Exception as e: print(f\"Error reading Buzzwords Excel: {e}\")\n",
        "\n",
        "print(\"\\n--- Finished Loading Data Sources ---\")\n",
        "\n",
        "\n",
        "# ===== 3. 全データの統合と前処理 (ツールセット適用) =====\n",
        "print(\"\\n--- Combining and Preprocessing All Documents (with Toolset) ---\")\n",
        "all_docs_raw = []\n",
        "source_info = [] # 各ドキュメントの出典情報を保持\n",
        "if cv_sections:\n",
        "    all_docs_raw.extend(cv_sections)\n",
        "    source_info.extend([{\"source_type\": \"cv\"}] * len(cv_sections))\n",
        "else:\n",
        "    print(\"Warning: Proceeding without CV sections.\")\n",
        "if journal_docs:\n",
        "    all_docs_raw.extend(journal_docs)\n",
        "    source_info.extend([{\"source_type\": \"journal\"}] * len(journal_docs))\n",
        "else:\n",
        "    print(\"Warning: Proceeding without journal entries.\")\n",
        "\n",
        "print(f\"Total raw documents combined: {len(all_docs_raw)}\")\n",
        "\n",
        "# spaCyモデルのロード\n",
        "print(\"Loading spaCy model...\")\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\") # または \"en_core_web_lg\" などより大きなモデル\n",
        "except OSError:\n",
        "    print(\"spaCy model not found. Please download it: python -m spacy download en_core_web_sm\")\n",
        "    nlp = None # エラーハンドリング\n",
        "\n",
        "# 注目ワードリストの前処理 (spaCyがロードされていれば)\n",
        "processed_watchlist_keywords = []\n",
        "if USE_WATCHLIST_ANALYSIS and WATCHLIST_KEYWORDS_RAW and nlp:\n",
        "    print(\"\\nPreprocessing Watchlist Keywords...\")\n",
        "    for kw_raw in WATCHLIST_KEYWORDS_RAW:\n",
        "        # preprocess_text_with_spacy はリストを返すので、通常は1要素のリスト\n",
        "        processed_kws = preprocess_text_with_spacy(kw_raw, nlp)\n",
        "        if processed_kws:\n",
        "            processed_watchlist_keywords.append(\" \".join(processed_kws)) # 複数トークンになる場合も考慮\n",
        "    print(f\"Processed Watchlist: {processed_watchlist_keywords}\")\n",
        "\n",
        "\n",
        "processed_docs_tokens = [] # トークンリストのリスト (N-gram処理前)\n",
        "processed_indices_map = []\n",
        "processed_doc_lengths_raw_tokens = []\n",
        "\n",
        "if all_docs_raw and nlp:\n",
        "    print(f\"Preprocessing {len(all_docs_raw)} documents with spaCy (POS, NER)...\")\n",
        "    for i, doc_text in enumerate(all_docs_raw):\n",
        "        tokens = preprocess_text_with_spacy(doc_text, nlp) # カスタマイズされた前処理関数\n",
        "        if tokens:\n",
        "            # ★長さフィルタ (例: 3トークン以上) - ここで調整可能★\n",
        "            min_tokens_threshold = 3\n",
        "            if len(tokens) >= min_tokens_threshold:\n",
        "                processed_docs_tokens.append(tokens) # トークンのリストとして保持\n",
        "                processed_indices_map.append(i)\n",
        "                processed_doc_lengths_raw_tokens.append(len(tokens))\n",
        "\n",
        "    print(f\"Initial spaCy preprocessing completed. {len(processed_docs_tokens)} documents retained.\")\n",
        "    if processed_doc_lengths_raw_tokens:\n",
        "        print(f\"Raw token counts per doc: Min={min(processed_doc_lengths_raw_tokens)}, Max={max(processed_doc_lengths_raw_tokens)}, Avg={sum(processed_doc_lengths_raw_tokens)/len(processed_doc_lengths_raw_tokens):.1f}\")\n",
        "\n",
        "    # N-gram生成 (USE_NGRAMSがTrueの場合)\n",
        "    # N-gram処理のために、一旦スペース区切りの文字列に戻す\n",
        "    docs_for_ngram = [\" \".join(tokens) for tokens in processed_docs_tokens]\n",
        "    if USE_NGRAMS:\n",
        "        print(\"\\nGenerating N-grams...\")\n",
        "        processed_docs_ngram_strings = generate_ngrams(docs_for_ngram, NGRAM_MIN_COUNT, NGRAM_THRESHOLD)\n",
        "        # N-gram処理後のトークンリストを再度作成 (BERTopicやTF-IDFは文字列リストを入力とするのでこのままでも良い)\n",
        "        # processed_docs = processed_docs_ngram_strings\n",
        "        # For BERTopic etc., string list is fine.\n",
        "        print(f\"N-gram generation completed. {len(processed_docs_ngram_strings)} documents.\")\n",
        "        # processed_docs_final は文字列のリスト\n",
        "        processed_docs_final = processed_docs_ngram_strings\n",
        "    else:\n",
        "        # processed_docs_final は文字列のリスト\n",
        "        processed_docs_final = docs_for_ngram\n",
        "\n",
        "    print(f\"\\nFinal preprocessing completed. Total documents for analysis: {len(processed_docs_final)}\")\n",
        "    if processed_docs_final: # processed_docs_final が空でないことを確認\n",
        "        final_doc_lengths = [len(doc.split()) for doc in processed_docs_final]\n",
        "        if final_doc_lengths: # final_doc_lengths が空でないことを確認\n",
        "            print(f\"Final token counts per doc (after ngrams if used): Min={min(final_doc_lengths)}, Max={max(final_doc_lengths)}, Avg={sum(final_doc_lengths)/len(final_doc_lengths):.1f}\")\n",
        "        else:\n",
        "            print(\"No documents after final processing, cannot calculate token counts.\")\n",
        "    else:\n",
        "        print(\"No documents available after final preprocessing.\")\n",
        "\n",
        "else:\n",
        "    print(\"No documents or spaCy model not loaded, skipping preprocessing.\")\n",
        "    processed_docs_final = []\n",
        "\n",
        "\n",
        "# ===== 3.5. 注目ワードリストの分析 (Watchlist Analysis) =====\n",
        "if USE_WATCHLIST_ANALYSIS and processed_watchlist_keywords and processed_docs_final:\n",
        "    print(\"\\n--- Analyzing Watchlist Keywords ---\")\n",
        "    watchlist_analysis_results = {}\n",
        "    for kw_proc in processed_watchlist_keywords:\n",
        "        containing_docs_indices = [\n",
        "            original_idx for doc_idx, original_idx in enumerate(processed_indices_map)\n",
        "            if kw_proc in processed_docs_final[doc_idx] # processed_docs_final を参照\n",
        "        ]\n",
        "        watchlist_analysis_results[kw_proc] = {\n",
        "            \"raw_keyword\": [r for r_idx, r in enumerate(WATCHLIST_KEYWORDS_RAW) if kw_proc in \" \".join(preprocess_text_with_spacy(r, nlp))], # 近似的なマッチ\n",
        "            \"processed_keyword\": kw_proc,\n",
        "            \"num_containing_docs\": len(containing_docs_indices),\n",
        "            \"containing_doc_original_indices\": containing_docs_indices\n",
        "        }\n",
        "        print(f\"Keyword '{kw_proc}': Found in {len(containing_docs_indices)} processed documents.\")\n",
        "        if len(containing_docs_indices) < 5 and len(containing_docs_indices) > 0: # 少数の場合、どの文書か表示\n",
        "            print(f\"  Original indices of containing docs: {containing_docs_indices}\")\n",
        "\n",
        "    # (オプション) さらに詳細な分析：TF-IDFスコア、BERTopicトピックなど\n",
        "\n",
        "\n",
        "# ===== 4. BERTopic 分析 (processed_docs_final を使用) =====\n",
        "topic_model = None; topics = None; probs = None; topic_info = None\n",
        "if processed_docs_final and len(processed_docs_final) > 1: # processed_docs_final に変更\n",
        "    print(\"\\n--- Running BERTopic Analysis ---\")\n",
        "    # (既存のBERTopicコード、入力は processed_docs_final)\n",
        "    try:\n",
        "        num_docs = len(processed_docs_final)\n",
        "        n_neighbors_val = min(30, max(5, num_docs // 10 + 1))\n",
        "        min_cluster_val = max(3, num_docs // 25 + 1)\n",
        "        min_topic_val = min_cluster_val\n",
        "        print(f\"Using BERTopic params: n_neighbors={n_neighbors_val}, min_cluster_size={min_cluster_val}, min_topic_size={min_topic_val}\")\n",
        "\n",
        "        umap_model = umap.UMAP(n_neighbors=n_neighbors_val, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
        "        hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=min_cluster_val, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
        "\n",
        "        topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True,\n",
        "                               min_topic_size=min_topic_val,\n",
        "                               umap_model=umap_model, hdbscan_model=hdbscan_model)\n",
        "\n",
        "        topics, probs = topic_model.fit_transform(processed_docs_final) # 入力を変更\n",
        "\n",
        "        if topic_model:\n",
        "            topic_info = topic_model.get_topic_info()\n",
        "            print(\"\\nBERTopic Results:\")\n",
        "            print(topic_info)\n",
        "            if topic_info is not None and len(topic_info) > 1:\n",
        "                print(\"\\nGenerating BERTopic visualizations...\")\n",
        "                for viz_func_name in [\"visualize_topics\", \"visualize_hierarchy\", \"visualize_barchart\", \"visualize_heatmap\"]:\n",
        "                     try:\n",
        "                         func = getattr(topic_model, viz_func_name)\n",
        "                         vis = func()\n",
        "                         if vis: filename = f\"combined_{viz_func_name}.html\"; vis.write_html(filename); print(f\"- {viz_func_name} saved to {filename}\")\n",
        "                     except Exception as e: print(f\"- Could not generate {viz_func_name}: {e}\")\n",
        "\n",
        "                # 注目ワードがどのトピックに属するか確認 (BERTopic後)\n",
        "                if USE_WATCHLIST_ANALYSIS and processed_watchlist_keywords and topics is not None:\n",
        "                    print(\"\\n--- Watchlist Keywords in BERTopic Topics ---\")\n",
        "                    # BERTopicは文書ごとにトピックを割り当てる。キーワード自体に直接トピックは振らない。\n",
        "                    # キーワードを含む文書がどのトピックに多いか、などで分析可能。\n",
        "                    # または、キーワードを短い文書としてtransformする。\n",
        "                    try:\n",
        "                        # キーワード自体を短い文書として扱い、トピックを予測\n",
        "                        watchlist_topics, _ = topic_model.transform(processed_watchlist_keywords)\n",
        "                        for i, kw_proc in enumerate(processed_watchlist_keywords):\n",
        "                            topic_id = watchlist_topics[i]\n",
        "                            topic_words = topic_model.get_topic(topic_id) if topic_id != -1 else [\"Outlier Topic\"]\n",
        "                            print(f\"Watchlist Keyword '{kw_proc}': Assigned to Topic {topic_id} ({topic_words[:5] if topic_words else 'N/A'})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Could not assign topics to watchlist keywords: {e}\")\n",
        "\n",
        "            else: print(\"\\nNo significant topics found by BERTopic (only outliers or failed).\")\n",
        "        else: print(\"BERTopic model fitting failed.\")\n",
        "    except Exception as e: print(f\"BERTopic failed: {e}\")\n",
        "else:\n",
        "    print(\"\\nSkipping BERTopic: Not enough processed documents or BERTopic disabled.\")\n",
        "\n",
        "# ===== 5. TF-IDF 分析 (processed_docs_final を使用) =====\n",
        "tfidf_top_keywords_per_doc_map = {}\n",
        "feature_names_tfidf = [] # TF-IDFの語彙\n",
        "df_tfidf = None\n",
        "if processed_docs_final and len(processed_docs_final) > 0: # processed_docs_final に変更\n",
        "    print(\"\\n--- Running TF-IDF Analysis ---\")\n",
        "    # (既存のTF-IDFコード、入力は processed_docs_final)\n",
        "    try:\n",
        "          vectorizer = TfidfVectorizer(min_df=1, max_df=1.0, stop_words=None, token_pattern=r\"(?u)\\b\\w[\\w-]*\\w\\b\") # N-gram対応のためtoken_pattern調整\n",
        "          tfidf_matrix = vectorizer.fit_transform(processed_docs_final) # 入力を変更\n",
        "          feature_names_tfidf = vectorizer.get_feature_names_out()\n",
        "          df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names_tfidf, index=processed_indices_map) # indexは元のまま\n",
        "          print(f\"TF-IDF matrix calculated ({df_tfidf.shape[0]} docs, {df_tfidf.shape[1]} features).\")\n",
        "\n",
        "          num_top_keywords = 7\n",
        "          for original_idx in df_tfidf.index: # df_tfidf.index は processed_indices_map の値\n",
        "               doc_tfidf_scores = df_tfidf.loc[original_idx]\n",
        "               top_keywords_indices = doc_tfidf_scores.nlargest(num_top_keywords).index\n",
        "               tfidf_top_keywords_per_doc_map[original_idx] = list(top_keywords_indices)\n",
        "          print(f\"Collected top {num_top_keywords} TF-IDF keywords for {len(tfidf_top_keywords_per_doc_map)} documents.\")\n",
        "\n",
        "          # 注目ワードのTF-IDFスコア確認\n",
        "          if USE_WATCHLIST_ANALYSIS and processed_watchlist_keywords and df_tfidf is not None:\n",
        "              print(\"\\n--- Watchlist Keywords TF-IDF Scores ---\")\n",
        "              for kw_proc in processed_watchlist_keywords:\n",
        "                  if kw_proc in feature_names_tfidf:\n",
        "                      # このキーワードを含む文書でのスコアの平均や最大値などを表示できる\n",
        "                      kw_scores = df_tfidf[kw_proc][df_tfidf[kw_proc] > 0]\n",
        "                      if not kw_scores.empty:\n",
        "                          print(f\"Keyword '{kw_proc}': Found in {len(kw_scores)} docs. Avg TF-IDF: {kw_scores.mean():.4f}, Max TF-IDF: {kw_scores.max():.4f}\")\n",
        "                      else:\n",
        "                          print(f\"Keyword '{kw_proc}': Found in TF-IDF vocab, but score is 0 in all processed docs.\")\n",
        "                  else:\n",
        "                      print(f\"Keyword '{kw_proc}': Not found in TF-IDF vocabulary.\")\n",
        "\n",
        "    except Exception as e: print(f\"TF-IDF failed: {e}\")\n",
        "else:\n",
        "    print(\"\\nSkipping TF-IDF: Not enough processed documents.\")\n",
        "\n",
        "\n",
        "# ===== 5.5. Word Embeddings (Sentence-BERT) と KeyBERT =====\n",
        "doc_embeddings = None\n",
        "sbert_model = None\n",
        "if USE_SENTENCE_BERT_EMBEDDINGS and processed_docs_final:\n",
        "    print(f\"\\n--- Generating Document Embeddings with Sentence-BERT ({SBERT_MODEL_NAME}) ---\")\n",
        "    try:\n",
        "        sbert_model = SentenceTransformer(SBERT_MODEL_NAME)\n",
        "        doc_embeddings = sbert_model.encode(processed_docs_final, show_progress_bar=True)\n",
        "        print(f\"Generated {doc_embeddings.shape[0]} embeddings of dimension {doc_embeddings.shape[1]}.\")\n",
        "\n",
        "        # (オプション) 注目ワードの埋め込みも生成\n",
        "        if USE_WATCHLIST_ANALYSIS and processed_watchlist_keywords:\n",
        "            watchlist_embeddings = sbert_model.encode(processed_watchlist_keywords)\n",
        "            # これらを使って類似度計算などが可能\n",
        "            # from sklearn.metrics.pairwise import cosine_similarity\n",
        "            # if doc_embeddings is not None and watchlist_embeddings is not None:\n",
        "            #     sim_matrix = cosine_similarity(watchlist_embeddings, doc_embeddings)\n",
        "            #     # print(f\"Similarity matrix between watchlist and docs:\\n{sim_matrix}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Sentence-BERT embedding generation failed: {e}\")\n",
        "\n",
        "keybert_model = None\n",
        "if USE_KEYBERT_EXTRACTION and processed_docs_final:\n",
        "    print(\"\\n--- Running KeyBERT Keyword Extraction ---\")\n",
        "    try:\n",
        "        # KeyBERTは内部でSentenceTransformerを使用するため、sbert_modelを渡すことも可能\n",
        "        if sbert_model:\n",
        "            keybert_model = KeyBERT(model=sbert_model) # 既存のSBERTモデルを再利用\n",
        "        else:\n",
        "            keybert_model = KeyBERT(model=SentenceTransformer(SBERT_MODEL_NAME)) # 新たにロード\n",
        "\n",
        "        keybert_top_keywords_per_doc_map = {}\n",
        "        for i, doc_text in enumerate(processed_docs_final):\n",
        "            # keyphrase_ngram_range で抽出するN-gramの範囲を指定\n",
        "            # top_n で上位N個\n",
        "            keywords_with_scores = keybert_model.extract_keywords(\n",
        "                doc_text,\n",
        "                keyphrase_ngram_range=(1, 3 if USE_NGRAMS else 1), # N-gram使用時は最大3-gramまで考慮\n",
        "                stop_words='english', # または None やカスタムリスト\n",
        "                top_n=7,\n",
        "                use_mmr=True, diversity=0.7 # MMRで多様性を上げる\n",
        "            )\n",
        "            if keywords_with_scores: # keywords_with_scores が空でないことを確認\n",
        "                keybert_top_keywords_per_doc_map[processed_indices_map[i]] = [kw[0] for kw in keywords_with_scores]\n",
        "            else: # keywords_with_scores が空の場合の処理\n",
        "                keybert_top_keywords_per_doc_map[processed_indices_map[i]] = []\n",
        "\n",
        "\n",
        "        print(f\"Collected top KeyBERT keywords for {len(keybert_top_keywords_per_doc_map)} documents.\")\n",
        "        # print(\"Sample KeyBERT keywords:\", dict(list(keybert_top_keywords_per_doc_map.items())[:2]))\n",
        "\n",
        "        # 既存の tfidf_top_keywords_per_doc_map をKeyBERTの結果で上書きするか、別途保持するか選択\n",
        "        # ここでは上書きせず、ネットワーク分析などでどちらを使うか選択できるようにする\n",
        "        # または、両方の結果を統合することも検討可能\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"KeyBERT extraction failed: {e}\")\n",
        "\n",
        "\n",
        "# ===== 6. NetworkX 共起ネットワーク分析 =====\n",
        "# NetworkXの対象とするキーワードリストを選択（TF-IDFベースかKeyBERTベースか、またはExcelリスト）\n",
        "network_target_keywords_source = \"excel\" # \"tfidf\", \"keybert\", \"excel\" から選択\n",
        "\n",
        "network_target_keywords = set()\n",
        "feature_names_for_network = [] # ネットワーク構築時の語彙（TF-IDF由来など）\n",
        "\n",
        "# Excelのキーワードを前処理 (既存のロジックを活かしつつ、spaCy前処理を適用)\n",
        "processed_excel_keywords_for_network = set()\n",
        "if my_keywords_from_list and nlp:\n",
        "    print(\"\\nPreprocessing Excel keywords for NetworkX with spaCy...\")\n",
        "    for kw_raw in my_keywords_from_list:\n",
        "        processed_kws_tokens = preprocess_text_with_spacy(str(kw_raw), nlp)\n",
        "        if processed_kws_tokens:\n",
        "            processed_excel_keywords_for_network.add(\" \".join(processed_kws_tokens)) # N-gram対応\n",
        "    print(f\"{len(processed_excel_keywords_for_network)} unique processed keywords from Excel list.\")\n",
        "\n",
        "\n",
        "if network_target_keywords_source == \"excel\":\n",
        "    # Excelリストのキーワードを、実際に文書中に出現する形（前処理後）に合わせる\n",
        "    # processed_docs_final に含まれる語彙と照合\n",
        "    temp_vocab_from_processed_docs = set(itertools.chain.from_iterable(doc.split() for doc in processed_docs_final))\n",
        "    network_target_keywords.update([kw for kw in processed_excel_keywords_for_network if kw in temp_vocab_from_processed_docs])\n",
        "    feature_names_for_network = list(temp_vocab_from_processed_docs) # ネットワークの語彙は全文書語彙\n",
        "    print(f\"Using {len(network_target_keywords)} keywords from processed Excel list (found in corpus) for NetworkX.\")\n",
        "elif network_target_keywords_source == \"tfidf\" and df_tfidf is not None:\n",
        "    # TF-IDFの上位語をターゲットにする (例: 全文書での合計TF-IDFスコアが高い上位N語)\n",
        "    if not df_tfidf.empty: # df_tfidf が空でないことを確認\n",
        "        sum_tfidf = df_tfidf.sum().sort_values(ascending=False)\n",
        "        network_target_keywords.update(sum_tfidf.head(100).index.tolist()) # 上位100語\n",
        "        feature_names_for_network = list(df_tfidf.columns)\n",
        "        print(f\"Using top {len(network_target_keywords)} TF-IDF keywords for NetworkX.\")\n",
        "    else:\n",
        "        print(\"TF-IDF data is empty, cannot select keywords for network.\")\n",
        "elif network_target_keywords_source == \"keybert\" and 'keybert_top_keywords_per_doc_map' in locals() and keybert_top_keywords_per_doc_map:\n",
        "    all_keybert_kws = set(itertools.chain.from_iterable(keybert_top_keywords_per_doc_map.values()))\n",
        "    network_target_keywords.update(all_keybert_kws)\n",
        "    # KeyBERTの場合、語彙は processed_docs_final から構築する必要がある\n",
        "    feature_names_for_network = list(set(itertools.chain.from_iterable(doc.split() for doc in processed_docs_final)))\n",
        "    print(f\"Using {len(network_target_keywords)} unique KeyBERT keywords for NetworkX.\")\n",
        "else:\n",
        "    print(\"Warning: Could not determine target keywords for NetworkX based on selection or data availability.\")\n",
        "\n",
        "# 注目ワードもネットワーク分析のターゲットに含めるか (オプション)\n",
        "if USE_WATCHLIST_ANALYSIS and processed_watchlist_keywords:\n",
        "    print(f\"Adding {len(processed_watchlist_keywords)} processed watchlist keywords to NetworkX targets.\")\n",
        "    network_target_keywords.update(processed_watchlist_keywords)\n",
        "    print(f\"Total target keywords for NetworkX (including watchlist): {len(network_target_keywords)}\")\n",
        "\n",
        "\n",
        "min_occurrence_network = 2\n",
        "G = None # Gを初期化\n",
        "if network_target_keywords and processed_docs_final:\n",
        "    print(f\"\\n--- Running NetworkX Analysis with {len(network_target_keywords)} target keywords ---\")\n",
        "    # (既存のNetworkXコード、入力は processed_docs_final と network_target_keywords)\n",
        "    co_occurrence_pairs = [];\n",
        "    # all_words_in_processed_docs は network_target_keywords に限定せず、全文書の語彙でカウントした方が良い場合もある\n",
        "    # ここでは、network_target_keywords に含まれる単語の出現回数をカウントする\n",
        "    all_target_words_in_docs = list(itertools.chain.from_iterable(\n",
        "        [word for word in doc.split() if word in network_target_keywords] for doc in processed_docs_final\n",
        "    ))\n",
        "    total_word_counts = Counter(all_target_words_in_docs)\n",
        "\n",
        "    for doc_text in processed_docs_final:\n",
        "        words_in_doc = set(doc_text.split())\n",
        "        keywords_in_doc = words_in_doc.intersection(network_target_keywords)\n",
        "        if len(keywords_in_doc) >= 2:\n",
        "            pairs_in_doc = [tuple(sorted(pair)) for pair in itertools.combinations(keywords_in_doc, 2)]\n",
        "            co_occurrence_pairs.extend(pairs_in_doc)\n",
        "\n",
        "    pair_counts = Counter(co_occurrence_pairs)\n",
        "    G = nx.Graph()\n",
        "\n",
        "    for pair, count in pair_counts.items():\n",
        "        if count >= min_occurrence_network:\n",
        "            keyword1, keyword2 = pair\n",
        "            # メタデータのキーも前処理後の形に合わせる必要がある\n",
        "            # keyword_metadata のキーは生のキーワードなので、前処理済みキーワードとのマッピングが必要\n",
        "            # ここでは簡易的に、前処理済みキーワードでノードを作成\n",
        "            meta1 = {} # keyword_metadata.get(raw_keyword_for_processed_k1, {})\n",
        "            meta2 = {} # keyword_metadata.get(raw_keyword_for_processed_k2, {})\n",
        "\n",
        "            if not G.has_node(keyword1):\n",
        "                G.add_node(keyword1, total_count=total_word_counts.get(keyword1, 1), **meta1)\n",
        "            if not G.has_node(keyword2):\n",
        "                G.add_node(keyword2, total_count=total_word_counts.get(keyword2, 1), **meta2)\n",
        "            G.add_edge(keyword1, keyword2, weight=count)\n",
        "\n",
        "    print(f\"Network graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges (min_occurrence={min_occurrence_network}).\")\n",
        "\n",
        "    # 描画 (既存)\n",
        "    if G.number_of_nodes() > 1 and G.number_of_edges() > 0:\n",
        "        # (既存の描画コード)\n",
        "        print(\"Drawing network graphs...\")\n",
        "        node_colors_cat = [] # カテゴリによる色分け\n",
        "        categories = set()\n",
        "        default_cat = 'Other (from text)'\n",
        "\n",
        "        # Excelのメタデータをノードにマッピングする改善が必要\n",
        "        # processed_excel_keywords_for_network と keyword_metadata のキーを紐付ける\n",
        "        # ここでは簡易的に、ノード名（処理済みキーワード）でメタデータを引く試み（失敗する可能性大）\n",
        "        for node in G.nodes():\n",
        "             cat = keyword_metadata.get(node, {}).get('cat_major', default_cat) # ノード名がExcelの元キーワードと一致する場合\n",
        "             if pd.isna(cat): cat = default_cat\n",
        "             node_colors_cat.append(cat); categories.add(cat)\n",
        "\n",
        "        unique_categories = sorted(list(categories))\n",
        "        # color_map_list = plt.cm.get_cmap('tab20', len(unique_categories)) if len(unique_categories) > 0 else plt.cm.get_cmap('tab20')\n",
        "        # color_map_dict = {cat: color_map_list(i) for i, cat in enumerate(unique_categories)}\n",
        "        # final_node_colors = [color_map_dict.get(cat, color_map_list(0)) for cat in node_colors_cat] # getでデフォルト色指定\n",
        "\n",
        "        # 色分けロジックを安全に\n",
        "        if unique_categories:\n",
        "            num_cats = len(unique_categories)\n",
        "            # cmap = plt.cm.get_cmap('viridis', num_cats if num_cats > 0 else 1) # カラーマップ変更 & num_catsが0の場合の対処\n",
        "            cmap = plt.get_cmap('viridis', num_cats if num_cats > 0 else 1)\n",
        "            color_map_dict = {category: cmap(i / num_cats if num_cats > 0 else 0) for i, category in enumerate(unique_categories)}\n",
        "        else: # カテゴリがない場合\n",
        "            color_map_dict = {default_cat: 'blue'} # デフォルトカテゴリに単一色\n",
        "            if not categories: # categoriesが空ならdefault_catを追加\n",
        "                categories.add(default_cat)\n",
        "                node_colors_cat = [default_cat] * G.number_of_nodes()\n",
        "\n",
        "\n",
        "        final_node_colors = [color_map_dict.get(cat, 'gray') for cat in node_colors_cat] # 取得できない場合はグレー\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(24, 24));\n",
        "        try: pos = nx.kamada_kawai_layout(G)\n",
        "        except: pos = nx.spring_layout(G, k=0.7, iterations=50, seed=42) # kパラメータ調整\n",
        "        node_sizes = [(G.degree(node) + 1) * 100 for node in G.nodes()] # サイズ調整\n",
        "        edge_widths = [G[u][v]['weight'] * 0.5 for u, v in G.edges()] # 太さ調整\n",
        "        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=final_node_colors, alpha=0.8)\n",
        "        nx.draw_networkx_edges(G, pos, width=edge_widths, edge_color='lightgray', alpha=0.5)\n",
        "        nx.draw_networkx_labels(G, pos, font_size=9, font_family='sans-serif') # フォントサイズ調整\n",
        "\n",
        "        if unique_categories: # unique_categories が空でないことを確認\n",
        "            legend_handles = [plt.Line2D([0], [0], marker='o', color='w', label=cat, markerfacecolor=color_map_dict.get(cat, 'gray'), markersize=10) for cat in unique_categories]\n",
        "            plt.legend(handles=legend_handles, title=\"Categories (Major)\", loc='best', fontsize='small')\n",
        "\n",
        "        plt.title(f\"Combined Data - Keyword Network by Category (Source: {network_target_keywords_source})\", size=20); plt.axis('off'); plt.tight_layout(); plt.show()\n",
        "        print(\"Graph drawing complete.\")\n",
        "    else: print(\"\\nCould not draw network graph based on current settings (not enough nodes/edges or G is None).\")\n",
        "else:\n",
        "    print(\"\\nSkipping NetworkX: Not enough target keywords or processed documents.\")\n",
        "    G = nx.Graph() # G が None のままにならないように空のグラフを代入\n",
        "\n",
        "# ===== 7. (オプション) メタデータ活用例 (既存コードにGの存在確認を強化) =====\n",
        "# (既存のメタデータ活用コード、GがNoneでないことを確認)\n",
        "if 'keyword_metadata' in locals() and keyword_metadata:\n",
        "     print(\"\\n--- Example: Analyzing Keyword Metadata from Excel ---\")\n",
        "     if G is not None and G.number_of_nodes() > 0: # G が None でないことを確認\n",
        "          categories_found_in_graph = {}\n",
        "          for node in G.nodes():\n",
        "             # ノード名は前処理済み。keyword_metadataのキーは生。ここのマッピングが課題。\n",
        "             # ここでは、もしノード名がkeyword_metadataにあれば、という仮定で進める。\n",
        "             cat = keyword_metadata.get(node, {}).get('cat_major', 'Unknown in Graph Meta')\n",
        "             if pd.isna(cat): cat = 'Unknown in Graph Meta'\n",
        "             categories_found_in_graph[cat] = categories_found_in_graph.get(cat, 0) + 1\n",
        "          print(\"Category distribution of NODES IN GRAPH (approximate due to key matching):\", categories_found_in_graph)\n",
        "     else:\n",
        "          print(\"NetworkX graph (G) was not created or has no nodes. Skipping category distribution for graph.\")\n",
        "\n",
        "     # Pillar情報の集計は G とは独立 (既存)\n",
        "     pillar_counts = {}\n",
        "     for kw_raw, meta in keyword_metadata.items(): # kw_raw は Excel の生のキーワード\n",
        "          if 'pillar' in meta and isinstance(meta['pillar'], list):\n",
        "               for p in meta['pillar']:\n",
        "                    if pd.notna(p):\n",
        "                         pillar_counts[p] = pillar_counts.get(p, 0) + 1\n",
        "     print(\"Keyword count per Pillar (from raw Excel list):\", pillar_counts)\n",
        "else:\n",
        "     print(\"\\nKeyword metadata not loaded. Skipping metadata analysis examples.\")\n",
        "\n",
        "print(\"\\n--- End of Optional Metadata Analysis ---\")\n",
        "\n",
        "print(\"\\n--- Comprehensive Analysis Workflow (with Toolset) Ready ---\")\n",
        "print(\"Please review the code, adjust file paths, parameters, and run the analysis.\")\n",
        "print(\"Consider installing: markdown-it-py, python-frontmatter, sentence-transformers, keybert\")"
      ]
    }
  ]
}